{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitda74fdf623974235942dd8c620d67e3f",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "metrics_to_calculate = ['auroc', 'aupr']\n",
    "\n",
    "gene_features = np.loadtxt('/Users/mac/OneDrive/thesis/scripts/dataset/ERN/X1.txt',delimiter=\",\")\n",
    "gene_TF_interaction_matrix = np.loadtxt('/Users/mac/OneDrive/thesis/scripts/dataset/ERN/Y.txt',delimiter=\",\")\n",
    "metric_values_per_fold = {}"
   ]
  },
  {
   "source": [
    "np.shape(gene_features)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1164, 445)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1164, 154)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "np.shape(gene_TF_interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================= Fold 0 =======================\n",
      "(117, 154)\n",
      "74 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 1 =======================\n",
      "(117, 154)\n",
      "77 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 2 =======================\n",
      "(117, 154)\n",
      "67 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 3 =======================\n",
      "(117, 154)\n",
      "83 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 4 =======================\n",
      "(116, 154)\n",
      "74 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 5 =======================\n",
      "(116, 154)\n",
      "80 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 6 =======================\n",
      "(116, 154)\n",
      "75 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 7 =======================\n",
      "(116, 154)\n",
      "84 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 8 =======================\n",
      "(116, 154)\n",
      "83 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "======================= Fold 9 =======================\n",
      "(116, 154)\n",
      "84 out of the 154 total labels has more than one classes present\n",
      "========================================================================\n",
      "\n",
      "auroc_micro: 0.9042554250191991 (0.011687330363991562)\n",
      "\n",
      "auroc_macro: 0.7490821377444645 (0.033442111197105336)\n",
      "\n",
      "aupr_micro: 0.5184487737147939 (0.027891080859796386)\n",
      "\n",
      "aupr_macro: 0.34968235325748404 (0.03323237988967994)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'auroc' in metrics_to_calculate:\n",
    "    metric_values_per_fold['auroc_micro'] = []\n",
    "    metric_values_per_fold['auroc_macro'] = []\n",
    "if 'aupr' in metrics_to_calculate:\n",
    "    metric_values_per_fold['aupr_micro'] = []\n",
    "    metric_values_per_fold['aupr_macro'] = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_counter = 0\n",
    "for train_index, test_index in kf.split(gene_features):\n",
    "    print('======================= Fold '+str(fold_counter)+' =======================')\n",
    "    \n",
    "    # split the dataset\n",
    "    X_train, X_test = gene_features[train_index], gene_features[test_index]\n",
    "    y_train, y_test = gene_TF_interaction_matrix[train_index], gene_TF_interaction_matrix[test_index]\n",
    "    \n",
    "    # define the oneVSrest classifier with the base classifier\n",
    "    clf = OneVsRestClassifier(RandomForestClassifier())\n",
    "    \n",
    "    #clf = OneVsRestClassifier(LogisticRegression(random_state=0))\n",
    "    \n",
    "    # fit the classifier on the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # generate probability predictions for every sample in the test set\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    print(str(y_pred.shape))\n",
    "    \n",
    "    # calculate the performance metrics on the test set\n",
    "    if 'auroc' in metrics_to_calculate:\n",
    "        metric_values_per_fold['auroc_micro'].append(roc_auc_score(y_test, y_pred, average='micro'))\n",
    "        \n",
    "        # This is not really important as we are only interested in the micro measures.\n",
    "        # Nevertheless, I basically do the macro averaging by hand so that I can skip labels that have only samples with one class\n",
    "        roc_auc_per_label = []\n",
    "        for label_idx in range(gene_TF_interaction_matrix.shape[1]):\n",
    "            if len(set(y_test[:, label_idx])) >= 2:\n",
    "                roc_auc_per_label.append(roc_auc_score(y_test[:, label_idx], y_pred[:, label_idx]))\n",
    "        print(str(len(roc_auc_per_label))+' out of the '+str(y_test.shape[1])+' total labels has more than one classes present')\n",
    "        \n",
    "        metric_values_per_fold['auroc_macro'].append(np.mean(roc_auc_per_label))\n",
    "\n",
    "        \n",
    "    if 'aupr' in metrics_to_calculate:\n",
    "        metric_values_per_fold['aupr_micro'].append(average_precision_score(y_test, y_pred, average='micro'))\n",
    "        \n",
    "        aupr_per_label = []\n",
    "        for label_idx in range(gene_TF_interaction_matrix.shape[1]):\n",
    "            if len(set(y_test[:, label_idx])) >= 2:\n",
    "                aupr_per_label.append(average_precision_score(y_test[:, label_idx], y_pred[:, label_idx]))\n",
    "        \n",
    "        metric_values_per_fold['aupr_macro'].append(np.mean(aupr_per_label))\n",
    "\n",
    "    \n",
    "    fold_counter += 1\n",
    "    print('========================================================================')\n",
    "    print('')\n",
    "    #calculate the mean and std for every metric measured during training and validation\n",
    "for metric_name in metric_values_per_fold.keys():\n",
    "    print(metric_name+': '+ str(np.mean(metric_values_per_fold[metric_name])) +' ('+ str(np.std(metric_values_per_fold[metric_name])) +')')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}